# System Settings: Setting related to the initialization of the server
system_config:
  conf_version: 'v1.1.1'
  host: '0.0.0.0' # use 0.0.0.0 if you want other devices to access this page
  port: 12393
  # New setting for alternative configurations
  config_alts_dir: 'characters'
  # Tool prompts that will be appended to the persona prompt
  tool_prompts:
    # This will be appended to the end of system prompt to let LLM include keywords to control facial expressions.
    # Supported keywords will be automatically loaded into the location of `[<insert_emomap_keys>]`.
    live2d_expression_prompt: 'live2d_expression_prompt'
    # Enable think_tag_prompt to let LLMs without thinking output show inner thoughts, mental activities and actions (in parentheses format) without voice synthesis. See think_tag_prompt for more details.
    # think_tag_prompt: 'think_tag_prompt'
  group_conversation_prompt: 'group_conversation_prompt' # When using group conversation, this prompt will be added to the memory of each AI participant.

# configuration for the default character
character_config:
  conf_name: 'shizuku-local' # The name of character configuration file.
  conf_uid: 'shizuku-local-001' # The unique identifier of character configuration.
  live2d_model_name: 'koakuma-event' # The name of Live2D model. Must be the same as the corresponding name in model_dict.json
  character_name: 'Koakuma' # Will be used in the group conversation and the display name of the AI.
  avatar: '' # Suggest using a square image for the avatar. Save it in the avatars folder. Leave blank to use the first letter of the character name as the avatar.
  human_name: 'Human' # Will be used in the group conversation and the display name of the human.


  persona_prompt: |
    I am your AI assistant.
    I respond with simple, concise answers.
    I keep answers very short and to the point.
    I never provide multiple answers to a single question.
    I never add unnecessary information.
    I use easy vocabulary.
    I never use square brackets [].
    I never use expressions in brackets to indicate my thoughts or assumptions.
    I never explain my thinking process.
    I respond directly with only one clear answer.
    I never say "I think" or similar phrases.
    No emoji or emoticons.

  #  =================== LLM Backend Settings ===================

  agent_config:
    conversation_agent_choice: 'basic_memory_agent'

    agent_settings:
      basic_memory_agent:
        # The Basic AI Agent. Nothing fancy.
        # choose one of the llm provider from the llm_config
        # and set the required parameters in the corresponding field
        # examples: 
        # 'openai_compatible_llm', 'llama_cpp_llm', 'claude_llm', 'ollama_llm'
        # 'openai_llm', 'gemini_llm', 'zhipu_llm', 'deepseek_llm', 'groq_llm'
        # 'mistral_llm'
        llm_provider: 'ollama_llm'
        # let ai speak as soon as the first comma is received on the first sentence
        # to reduced latency.
        faster_first_response: true
        # Method for segmenting sentences: 'regex' or 'pysbd'
        segment_method: 'pysbd'

      mem0_agent:
        vector_store:
          provider: 'qdrant'
          config:
            collection_name: 'test'
            host: 'localhost'
            port: 6333
            embedding_model_dims: 1024

        # mem0 has it's own llm settings and is different from our llm_config.
        # check their docs for more details
        llm:
          provider: 'ollama'
          config:
            model: 'llama3.1:latest'
            temperature: 0
            max_tokens: 8000
            ollama_base_url: 'http://localhost:11434'

        embedder:
          provider: 'ollama'
          config:
            model: 'mxbai-embed-large:latest'
            ollama_base_url: 'http://localhost:11434'
      hume_ai_agent:
        api_key: ''
        host: 'api.hume.ai' # Do not change this in most cases
        config_id: '' # Optional
        idle_timeout: 15 # How many seconds to wait before disconnecting
      # MemGPT Configurations: MemGPT is temporarily removed
      ##

    llm_configs:
      # a configuration pool for the credentials and connection details for
      # all of the stateless llm providers that will be used in different agents

      # OpenAI Compatible inference backend
      openai_compatible_llm:
        base_url: 'http://localhost:11434/v1'
        llm_api_key: 'somethingelse'
        organization_id: 'org_eternity'
        project_id: 'project_glass'
        model: 'qwen2.5:latest'
        temperature: 1.0 # value between 0 to 2
        interrupt_method: 'user'
        # This is the method to use for prompting the interruption signal. 
        # If the provider supports inserting system prompt anywhere in the chat memory, use 'system'. 
        # Otherwise, use 'user'. You don't usually need to change this setting.

      # Claude API Configuration
      claude_llm:
        base_url: 'https://api.anthropic.com'
        llm_api_key: 'YOUR_API_KEY_HERE'
        model: 'claude-3-haiku-20240307'

      llama_cpp_llm:
        model_path: '<path-to-gguf-model-file>'
        verbose: false

        n_gpu_layers: -1
        n_batch: 512
        cache_capacity: 2000
        f16_kv: true
        use_mlock: true
        seed: 42
      ollama_llm:
        base_url: 'http://localhost:11434/v1'
        model: 'mistral:7b-instruct'
        temperature: 0.5
        # seconds to keep the model in memory after inactivity. 
        # set to -1 to keep the model in memory forever (even after exiting open llm vtuber)
        keep_alive: -1
        unload_at_exit: true # unload the model from memory at exit

        max_tokens: 1024
        top_k: 40
        top_p: 0.95
        repeat_penalty: 1.1
        num_gpu: 1
        num_thread: 8
        mirostat: 0
        use_cache: true
      openai_llm:
        llm_api_key: 'YOUR_API_KEY_HERE'
        model: 'gpt-3.5-turbo'
        temperature: 1.0 # value between 0 to 2

        max_tokens: 1024
        top_p: 0.95
      gemini_llm:
        llm_api_key: 'YOUR_API_KEY_HERE'
        model: 'gemini-2.0-flash-exp'
        temperature: 1.0 # value between 0 to 2

      zhipu_llm:
        llm_api_key: 'YOUR_API_KEY_HERE'
        model: 'glm-4-flash'
        temperature: 1.0 # value between 0 to 2

      deepseek_llm:
        llm_api_key: 'YOUR_API_KEY_HERE'
        model: 'deepseek-chat'
        temperature: 0.7 # note that deepseek's temperature ranges from 0 to 1
      mistral_llm:
        llm_api_key: 'YOUR_API_KEY_HERE'
        model: 'pixtral-large-latest'
        temperature: 1.0 # value between 0 to 2

      groq_llm:
        llm_api_key: 'YOUR_API_KEY_HERE'
        model: 'llama-3.3-70b-versatile'
        temperature: 1.0 # value between 0 to 2

  # === Automatic Speech Recognition ===
  asr_config:
    # speech to text model options: 'faster_whisper', 'whisper_cpp', 'whisper', 'azure_asr', 'fun_asr', 'groq_whisper_asr', 'sherpa_onnx_asr'
    asr_model: 'azure_asr'

    azure_asr:
      api_key: 'YOUR_AZURE_API_KEY_HERE'
      region: 'eastus'
      languages: ['en-US', 'zh-CN', 'ja-JP', 'ko-KR'] # List of languages to detect

    # Faster whisper config
    faster_whisper:
      model_path: 'distil-medium.en' # distil-medium.en is an English-only model
      #                               use distil-large-v3 if you have a good GPU
      download_root: 'models/whisper'
      language: 'auto' # en, zh, or something else. put nothing for auto-detect.
      device: 'auto' # cpu, cuda, or auto. faster-whisper doesn't support mps

    whisper_cpp:
      # all available models are listed on https://abdeladim-s.github.io/pywhispercpp/#pywhispercpp.constants.AVAILABLE_MODELS
      model_name: 'small'
      model_dir: 'models/whisper'
      print_realtime: false
      print_progress: false
      language: 'auto' # en, zh, auto,

    whisper:
      name: 'medium'
      download_root: 'models/whisper'
      device: 'cpu'

    # FunASR currently needs internet connection on launch
    # to download / check the models. You can disconnect the internet after initialization.
    # Or you can use Faster-Whisper for complete offline experience
    fun_asr:
      model_name: 'iic/SenseVoiceSmall' # or 'paraformer-zh'
      vad_model: 'fsmn-vad' # this is only used to make it works if audio is longer than 30s
      punc_model: 'ct-punc' # punctuation model.
      device: 'cpu'
      disable_update: true # should we check FunASR updates everytime on launch
      ncpu: 4 # number of threads for CPU internal operations.
      hub: 'ms' # ms (default) to download models from ModelScope. Use hf to download models from Hugging Face.
      use_itn: false
      language: 'auto' # zh, en, auto

    # pip install sherpa-onnx
    # documentation: https://k2-fsa.github.io/sherpa/onnx/index.html
    # ASR models download: https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
    sherpa_onnx_asr:
      model_type: 'sense_voice' # 'transducer', 'paraformer', 'nemo_ctc', 'wenet_ctc', 'whisper', 'tdnn_ctc'
      #  Choose only ONE of the following, depending on the model_type:
      # --- For model_type: 'transducer' ---
      # encoder: ''        # Path to the encoder model (e.g., 'path/to/encoder.onnx')
      # decoder: ''        # Path to the decoder model (e.g., 'path/to/decoder.onnx')
      # joiner: ''         # Path to the joiner model (e.g., 'path/to/joiner.onnx')
      # --- For model_type: 'paraformer' ---
      # paraformer: ''     # Path to the paraformer model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'nemo_ctc' ---
      # nemo_ctc: ''        # Path to the NeMo CTC model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'wenet_ctc' ---
      # wenet_ctc: ''       # Path to the WeNet CTC model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'tdnn_ctc' ---
      # tdnn_model: ''      # Path to the TDNN CTC model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'whisper' ---
      # whisper_encoder: '' # Path to the Whisper encoder model (e.g., 'path/to/encoder.onnx')
      # whisper_decoder: '' # Path to the Whisper decoder model (e.g., 'path/to/decoder.onnx')
      # --- For model_type: 'sense_voice' ---
      # I've coded so that the sense voice model will get automatically downloaded.
      # For other models, you need to download them yourself
      sense_voice: './models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/model.int8.onnx' # Path to the SenseVoice model (e.g., 'path/to/model.onnx')
      tokens: './models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/tokens.txt' # Path to tokens.txt (required for all model types)
      # --- Optional parameters (with defaults shown) ---
      # hotwords_file: ''     # Path to hotwords file (if using hotwords)
      # hotwords_score: 1.5   # Score for hotwords
      # modeling_unit: ''     # Modeling unit for hotwords (if applicable)
      # bpe_vocab: ''         # Path to BPE vocabulary (if applicable)
      num_threads: 4 # Number of threads
      # whisper_language: '' # Language for Whisper models (e.g., 'en', 'zh', etc. - if using Whisper)
      # whisper_task: 'transcribe'  # Task for Whisper models ('transcribe' or 'translate' - if using Whisper)
      # whisper_tail_paddings: -1   # Tail padding for Whisper models (if using Whisper)
      # blank_penalty: 0.0    # Penalty for blank symbol
      # decoding_method: 'greedy_search'  # 'greedy_search' or 'modified_beam_search'
      # debug: False # Enable debug mode
      # sample_rate: 16000 # Sample rate (should match the model's expected sample rate)
      # feature_dim: 80       # Feature dimension (should match the model's expected feature dimension)
      use_itn: true # Enable ITN for SenseVoice models (should set to False if not using SenseVoice models)
      # Provider for inference (cpu or cuda) (cuda option needs additional settings. Please check our docs)
      provider: 'cpu'

    groq_whisper_asr:
      api_key: 'YOUR_API_KEY_HERE'
      model: 'whisper-large-v3-turbo' # or 'whisper-large-v3'
      lang: ''

  # =================== Text to Speech ===================
  tts_config:
    # Supported tts_model options: 'azure_tts', 'edge_tts', 'coqui_tts', 'bark_tts', 'cosyvoice_tts', 'cosyvoice2_tts', 'melo_tts', 'x_tts', 'gpt_sovits_tts', 'fish_api_tts', 'sherpa_onnx_tts', 'voicevox_tts'
    tts_model: 'voicevox_tts'
    voicevox_tts:
      host: "127.0.0.1"
      port: 50021
      speaker_id: 46
      speed_scale: 1.0
      pitch_scale: 0.0
      intonation_scale: 1.0
      volume_scale: 1.0
      use_korean_preprocessing: true  # 한국어 전처리 활성화

    # edge tts is a light tts engine that uses Microsoft Edge's TTS.
    # It's free and does not require any API key.
    edge_tts:
      voice: 'ko-KR-SeoHyeonNeural'
      rate: '+0%'
      volume: '+0%'
      # Supports subtitles and audio transcripts
      # True to enable subtitles for audio
      subtitle: true

    azure_tts:
      api_key: 'YOUR_API_KEY_HERE'
      region: 'eastus'
      voice: 'en-US-AshleyNeural'
      pitch: '26' # percentage of the pitch adjustment
      rate: '1' # rate of speak

    bark_tts:
      voice: 'v2/en_speaker_1'

    # pyttsx3_tts doesn't have any config.

    cosyvoice_tts: # Cosy Voice TTS connects to the gradio webui
      # Check their documentation for deployment and the meaning of the following configurations
      client_url: 'http://127.0.0.1:50000/' # CosyVoice gradio demo webui url
      mode_checkbox_group: ''